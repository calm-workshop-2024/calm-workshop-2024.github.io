<!DOCTYPE html>
<html lang="en"><head>

    <meta name="generator" content="Hugo 0.101.0" />
    <meta name="date" content="">
    
    <meta charset="utf-8">
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="referrer" content="no-referrer">
    
    <meta name="author" content="Felix Leeb, Ching Lam Choi, Luigi Gresele, Josef Valvoda, Andrei Nicolicioiu, Xiusi Li, Patrik Reizinger, Sophie Xhonneux, Haoxuan Li, Mengyue Yang, Bernhard Schölkopf, Dhanya Sridhar" />
    <meta name="description" content="First Workshop on Causality and Large Models (C♥️LM) @ NeurIPS 2024" />
    <meta name="keywords" content="workshop, causal representation learning, large models" />
    
    <title>C♥️LM @ NeurIPS 2024 | Home</title>
    
    <meta property="og:title" content="Home" />
    <meta property="og:type" content="website" />
    <meta property="og:description" content="First Workshop on Causality and Large Models (C♥️LM) @ NeurIPS 2024" />
    
    <meta name="twitter:title" content="" />
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,300;0,400;1,300&display=swap" rel="stylesheet"> 
    
    <link rel="canonical" href="https://calm-workshop-2024.github.io/">
    <link rel="stylesheet" href="https://calm-workshop-2024.github.io/styles.css">
    
    <link rel="apple-touch-icon" sizes="180x180" href="https://calm-workshop-2024.github.io/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://calm-workshop-2024.github.io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://calm-workshop-2024.github.io/favicon-16x16.png">
    <link rel="manifest" href="https://calm-workshop-2024.github.io/site.webmanifest">

</head><body><section id="header">

    <div id="logo-container">
        <div id="title-inner-container">
            <a href="https://calm-workshop-2024.github.io/"><img src="https://calm-workshop-2024.github.io/logo3.jpg" id="logo" width="180px"></a>
        </div>
    </div>

    <div id="title-container">
        <div id="title-inner-container">
            <div class="supertitle">C♥️LM: First Workshop on</div>
            <div class="title"><a href="https://calm-workshop-2024.github.io/">Causality and Large Models</a></div>
            <div class="subtitle">December @ NeurIPS 2024</div>
        </div>
    </div>

    <br>

    <div id="navigation">
        <ul>
        
            <li><strong><a href="https://calm-workshop-2024.github.io/call-for-papers">Call for Papers</a></strong></li>
        
            <li><strong><a href="https://calm-workshop-2024.github.io/about">About</a></strong></li>
        
            <li><strong><a href="https://calm-workshop-2024.github.io/dates">Dates</a></strong></li>
        
            <li><strong><a href="https://calm-workshop-2024.github.io/schedule">Schedule</a></strong></li>
        
            <li><strong><a href="https://calm-workshop-2024.github.io/team">Our Team</a></strong></li>
        
            <li><strong><a href="https://calm-workshop-2024.github.io/invited-speakers">Invited Speakers</a></strong></li>
        
      </ul>
    </div>

</section><section id="content">
        
    <h1 id="welcome">Welcome!</h1>
<!-- <img src="logo.jpeg" alt="CALM workshop" width="500px"/></a> -->
<p>The <strong>First Workshop on Workshop on
Causality and Large Models (C♥️LM)</strong> will be co-located with <a href="https://neurips.cc/Conferences/2024/"><strong>NeurIPS 2024</strong></a>, in Vancouver, and held on <strong>December, 2024.</strong></p>
<h2 id="about-the-workshop">About the workshop</h2>
<p>The remarkable capabilities and accessibility of recent large models, also known as “foundation models,” have sparked significant interest and excitement in the research community and beyond. In particular, large pre-trained generative models have demonstrated remarkable competencies in understanding and generating human-like text despite being trained on largely unstructured data using relatively simple self-supervised learning objectives. This raises the question: (A) <em>Why do such large models work so well?</em></p>
<p>The impressive performance, sometimes even exceeding human experts, across a wide variety of benchmarks, together with the incorporation of multiple modalities such as images, text, and audio, makes these large models particularly versatile decision-making systems. However, the increased adoption of these models is not without challenges. The increasing size and complexity of these “black box” models raises concerns about their trustworthiness and reliability. For real-world applications, where distribution shifts are pervasive and sufficient high-quality data may be difficult or expensive to collect, it is crucial to systematically verify and enhance the robustness and generalization capabilities of these models. This is especially pertinent in safety-critical domains, such as healthcare and policy-making. Consequently, we must consider: (B) <em>Under what circumstances can we trust these large models and how can this be improved?</em></p>
<p>Enter causality: a systematic framework to formalize “why?” and “how?” questions much like (A) or (B) and develop principled tools to address them. Causal inference is a powerful approach to describe a system’s behavior under interventions and reason over counterfactual scenarios. By relying on stable causal relationships, instead of potentially spurious statistical correlations, causal models can transparently elucidate a system’s behavior and enable performance guarantees beyond the training distribution, which is crucial for high-risk applications. However, translating the rigorous theoretical tools of causality into practical methods, especially in the large-scale regime with heterogeneous unstructured data as in large models, remains a notable challenge, despite the growing attention by the community.</p>
<p>With the striking potential of causality and the enormous interest in tackling the many open questions about understanding and improving large models on the other, we propose a workshop that aims to explore the many exciting synergies between causality and large models. Specifically, we identify four main directions to cover in our workshop:</p>
<ul>
<li>
<p>Causality in large models: Assessing the causal knowledge captured by large models and their (causal) reasoning abilities.</p>
</li>
<li>
<p>Causality for large models: Applying ideas from causality to augment and improve large models.</p>
</li>
<li>
<p>Causality with large models: Leveraging large models to improve causal inference and discovery.</p>
</li>
<li>
<p>Causality of large models: Investigating the causal structure of how large models work and how to make them more interpretable and controllable.</p>
</li>
</ul>
<h2 id="important-dates">Important Dates</h2>
<ul>
<li><strong>Paper submission deadline: September 06, 2024, AoE</strong></li>
<li><strong>Notification to authors: October 09, 2024, AoE</strong></li>
<li><strong>Camera-ready version: TBA</strong></li>
<li><strong>Workshop Date: December 14 or 15, 2024 - TBD, AoE</strong></li>
</ul>
<p>Contact us at <a href="mailto:calmworkshop2024@gmail.com">calmworkshop2024@gmail.com</a></p>


    </section>
<div id="footer">
    Made with <a href="https://gohugo.io/">Hugo</a> and hosted on <a href="TODO">GitHub</a>.
</div>


</body>

</html>