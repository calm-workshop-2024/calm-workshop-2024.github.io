---
title: "Call for Papers"
---

# Call for Papers

With the striking potential of causality and the enormous interest in tackling the many open questions about understanding and improving large models on the other, we propose a workshop that aims to explore the many exciting synergies between causality and large models. Specifically, we identify four main directions to cover in our workshop:

* Causality in large models: Assessing the causal knowledge captured by large models and their (causal) reasoning abilities.

* Causality for large models: Applying ideas from causality to augment and improve large models.

* Causality with large models: Leveraging large models to improve causal inference and discovery.

* Causality of large models: Investigating the causal structure of how large models work and how to make them more interpretable and controllable.

By engaging both theoretical and applied perspectives, we aim to foster deep insights into the behavior of large models, not only to assess their current capabilities but also to improve their performance and to make their behavior more interpretable and reliable. In addition to the directions discussed above, we welcome contributions that explore the vibrant interface between causality and large models, for example:

* Theoretical and empirical studies on how large models employ causal knowledge and reasoning

* Combined applications of large models and causal methods to real-world problems for improved performance and interpretability

* Connections between multiple modalities (such as images, text, and audio) and (causal) reasoning abilities of large models

* Applications of causal inference methods to understand or improve large models

<!-- We welcome submissions related to any aspects of CRL, including but not limited to:

* Learning latent (structural) causal models & structured (deep) generative models
* Interventional representations, causal digital twins & structured (causal) world models
* Post-hoc extraction of causal relations from (deep) generative models
* Self-supervised causal representation learning
* Multi-environment & multi-view causal representation learning
* Micro vs. macro/coarse-grained/multi-level causal systems
* Identifiable representation learning & nonlinear ICA
* Uncertainty quantification in (causal) representation learning
* Group-theoretic & symmetry-based views on disentanglement
* Invariance & equivariance in representation learning
* Interdisciplinary perspectives on causal representation learning, including from cognitive science, psychology, (computational) neuroscience or philosophy
* Real-world applications of causal representation learning, including in biology, medical sciences, or robotics


Submissions should present novel, unpublished work. Work that previously appeared in non-archival venues (such as arXiv or other workshops without proceedings) is allowed.

The CRL workshop is non-archival, and should thus generally not violate dual submission policies at other archival venues (e.g., submitting work that is currently under review at another conference such as NeurIPS is permitted); if unsure, please check yourself with the corresponding venue. -->
